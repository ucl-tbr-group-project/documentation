\begin{frame}
	\frametitle{Outline}
	\begin{itemize}
		\item % TODO
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Experiments 1 \& 2: Hyperparameter Tuning}

	\includegraphics[height=95pt]{exp1_slice0}\hfill%
	\includegraphics[height=95pt]{exp1_slice1}\hfill%
	\includegraphics[height=95pt]{exp1_slice2}

	\begin{columns}
		\column{0.28\textwidth}
		\includegraphics[height=95pt]{exp2_time_vs_reg}

		\column{0.72\textwidth}
		\begin{itemize}
			\item
				Experiment 1 (3x top), Experiment 2 (left).
			\item
				Showing the best 20 surrogates per family.
			\item
				Omitting discrete features yields only a negligible
				improvement in performance.
			\item
				Overall dominated by tree-based surrogates (GBTs, ERTs) and
				neural networks.
		\end{itemize}
	\end{columns}

\end{frame}

\begin{frame}
	\frametitle{Experiment 3: Scaling Benchmark}
	\begin{columns}
		\column{0.5\textwidth}
		\begin{itemize}
			\item
				We observe a hierarchy.
			\item
				Best-performing families from the previous experiments also scale the
				best in $t_\text{pred}$.
			\item
				More samples: neural networks outperform tree-based models.
		\end{itemize}

		\column{0.5\textwidth}
		\begin{itemize}
			\item
				Instance-based surrogates offer train trivially but have
				complex lookup strategies.
			\item
				Neural networks show inverse scaling due to
				parallelization.
		\end{itemize}
	\end{columns}

	\vspace{1em}

	\includegraphics[height=95pt]{scaling_metric_r2}
	\includegraphics[height=95pt]{scaling_time_train}
	\includegraphics[height=95pt]{scaling_time_pred}
\end{frame}

\begin{frame}
	\frametitle{Experiment 4: Model Comparison}
	\begin{itemize}
		\item % TODO
	\end{itemize}
\end{frame}

We employed a broad spectrum of data
analysis and machine learning techniques to develop fast and high-quality
surrogates for the Paramak TBR model. After reviewing
9~surrogate model families, examining their behaviour on a constrained and
unrestricted feature space, and studying their scaling properties, we retrained
the best-performing instances to produce properties desirable for
practical use. The fastest surrogate, an artificial neural network trained
on~\num{500000} datapoints, attained an~$R^2=\num{0.985}$ with mean prediction time
of~$\SI{0.898}{\micro\second}$, representing a relative
speedup of $8\cdot 10^6$ with respect to the MC model. We demonstrated the possibility of achieving comparable results using only a
training set of size~\num{10000}.

We further developed a novel adaptive
sampling algorithm, QASS, capable of interfacing with any surrogate model.
Preliminary testing on a toy theory, qualitatively comparable to
the MC TBR model, demonstrated the effectiveness of QASS and behavioural trends
consistent with the design of the algorithm. With~\num{100000} initial samples
and 100 incremental samples per iteration, QASS achieved a ${\sim}40\%$ decrease
in surrogate error compared to a baseline random sampling scheme. Further optimisation over the hyperparameter space has strong potential to increase this performance, in particular by decreasing the required quantity of initial samples. This will allow for future deployment of QASS on the Paramak TBR model in coalition with any of the most effective identified surrogates.

Relevant source code, model instances and datasets are freely
available online as well as a more detailed technical
report~\cite{github,finalreport}.

Labeling the expensive Paramak model $f(x)$, a surrogate is a mapping
$\hat{f}(x)$ such that $f(x)$ and $\hat{f}(x)$ minimize a selected dissimilarity
metric. In order to be considered \textit{viable}, $\hat{f}(x)$ is required to
achieve expected evaluation time lower than that of~$f(x)$. In this work, we
consider 2~methods of producing viable surrogates: (1)~a conventional decoupled
approach, which evaluates $f(x)$ on a set of randomly sampled points and
trains surrogates in a supervised scheme, and (2)~an adaptive approach, which attempts to
compensate for localized regression performance insufficiency's by interleaving
multiple epochs of sampling and training.

\begin{table}[t]
	\setlength\tabcolsep{1pt}
	\renewcommand{\arraystretch}{0.95}
	\caption{\label{tbl:surrogates}Considered surrogate model families, their
		selected abbreviations and implementations. $\mathcal{H}$~denotes the
		set of hyperparameters.}
	\begin{indented}
	\item[]
		\begin{tabular}{lllr}
		\toprule
		Surrogate family & Abbr. & Impl. & $|\mathcal{H}|$ \\
		\midrule
		Support vector machines~\cite{fan2008liblinear}	& SVM & SciKit~\cite{scikit-learn} & 3 \\
		Gradient boosted trees~\cite{friedman2001greedy,friedman1999stochastic,hastie2009elements}	& GBT & SciKit & 11 \\
		Extremely randomized trees~\cite{geurts2006extremely}	& ERT & SciKit & 7 \\
		AdaBoosted decision trees$^\text{a}$~\cite{drucker1997improving}	& ABT & SciKit & 3 \\
		Gaussian process regression~\cite{williams2006gaussian}	& GPR & SciKit & 2 \\
		$k$ nearest neighbours	& KNN & SciKit & 3 \\
		Artificial neural networks	& ANN & Keras~\cite{chollet2015keras} & 2 \\
		Inverse distance weighing~\cite{shepard1968two} & IDW & SMT~\cite{SMT2019} & 1 \\
		Radial basis functions & RBF & SMT & 3 \\
		\bottomrule
		\end{tabular}\\%
		{\footnotesize $^\text{a}$Note that ABTs can be viewed as a subclass of GBTs.}
	\end{indented}
\end{table}

\begin{table*}[t]
	\renewcommand{\arraystretch}{0.95}
	\caption{\label{tbl:metrics}Metrics recorded in experiments. In
	formulations, we work with a training set of size $N_0$ and a test set of
size $N$, values $y^{(i)}=f(x^{(i)})$ and $\hat{y}^{(i)}=\hat{f}(x^{(i)})$
denote images of the $i$th testing sample in Paramak and the surrogate
respectively. The mean $\overline{y}=N^{-1}\sum_{i=1}^N y^{(i)}$ and $P$ is the
number of input features.}
	\begin{indented}
	\item[]
		\begin{tabularx}{\textwidth}{Xrl}
		\toprule
		Regression performance metrics& Notation	& Mathematical formulation\\
		\midrule
		Mean absolute error	& MAE & $N^{-1}\sum_{i=1}^N |y^{(i)}-\hat{y}^{(i)}|$ \\
		Standard deviation of error & $S$	& $\text{StdDev}_{i=1}^N\left\{ |y^{(i)} -
		\hat{y}^{(i)}| \right\} $ \\
			Coefficient of determination & $R^2$	& $1-\sum_{i=1}^N
			\left(y^{(i)}-\hat{y}^{(i)} \right)^2\left[\sum_{i=1}^N \left(
			y^{(i)}-\overline{y} \right)^2\right]^{-1} $ \\
			Adjusted $R^2$ & $R^2_\text{adj.}$	& $1-(1-R^2)(N-1)(N-P-1)^{-1}$ \\
		\midrule
		Computational complexity metrics	& {}	& {} \\
		\midrule
		Mean training time & $\overline{t}_{\text{trn.}}$	& $(\text{wall training time of
		$\hat{f}(x)$})N_0^{-1}$  \\
			Mean prediction time & $\overline{t}_{\text{pred.}}$	& $(\text{wall prediction time of
			$\hat{f}(x)$})N^{-1}$ \\
				Relative speedup & $\omega$	& $(\text{wall evaluation$^\text{b}$ time of $f(x)$})
				(N\overline{t}_{\text{pred.}})^{-1}$ \\
		\bottomrule
		\end{tabularx}\\%
		{\footnotesize $^\text{b}$This corresponds to evaluation of Paramak
		 on all points of the test set. In surrogates, the equivalent
		time period is referred to as ``prediction time.''}
	\end{indented}
\end{table*}

For both methods, we selected state-of-the-art regression algorithms to perform
surrogate training on sampled point sets. Listed in~\Tref{tbl:surrogates}, these
implementations define 9~surrogate families that are later reviewed in~\Sref{sec:results}.
We note that each presented algorithm defines hyperparameters that may influence its
performance. Their problem-specific optimal values are searched within the scope
of this work, in particular in Experiments~1 \&~2 that are outlined
in~\Sref{sec:experiment-methodology}.

To compare the quality of the produced surrogates, we define a number of metrics listed
in~\Tref{tbl:metrics}. For regression performance analysis, we include a
selection of absolute metrics to assess the models' approximation capability and set
practical bounds on the expected uncertainty of their predictions. In addition, we also track
relative measures that are better-suited for comparison between this work and others as
they are invariant with respect to the selected domain and image space.
For analysis of computational complexity, surrogates are assessed in terms of wall
time (captured by the Python~3 \texttt{time} package). This is motivated by common practical use
cases of our work, where models are trained and used as drop-in replacements for
Paramak. All times reported (training, test, evaluation) are
normalized by the corresponding dataset size, i.e.~correspond to ``time to
process a single datapoint.''

Even though some surrogates support acceleration by means of parallelisation, we
used non-parallelized implementations. The only exception to this are ANNs,
which require a considerable amount of processing power for training on
conventional CPU architectures. Lastly, to prevent undesirable bias by training
set selection, all reported metrics are obtained via 5-fold cross-validation.
In this setting, a sample set is uniformly divided into 5 disjoint folds, each of which
is used as a test set for models trained on the remaining 4. Having repeated the
same experiment for each division, the overall value of individual metrics is
reported in terms of their mean and standard deviation over all folds.



\subsection{Decoupled Approach}\label{sec:experiment-methodology}

Experiments related to the decoupled approach are organized in 4~parts,
further described in this section. In summary, we aim to optimize the hyperparameters of
each surrogate family separately and later use the best found results to compare surrogate
families among themselves.

The objective of Experiment~1 is to simplify the regression task for
surrogates prone to suboptimal performance in discontinuous spaces.
To this end, training points are filtered to a single selected discrete feature
assignment, and surrogates are trained only on the remaining continuous features.
This is repeated several times to explore variances in behaviour,
particularly in 4~distinct assignments that are obtained by setting blanket and
first wall coolant materials to one
of:~$\{\text{H\textsubscript{2}O},\isotope{He}\}$.
Experiment~2 conventionally measures surrogate performance on the full feature
space without any parameter restrictions. In both experiments, hyperparameter tuning is
facilitated by Bayesian optimisation~\cite{movckus1975bayesian}, where we select the
hyperparameter configuration that produces the model that maximizes $R^2$. The
process is terminated after 1000~iterations or 2~days, whichever condition is satisfied first.
The results of Experiments~1 \&~2 are depicted
in Figures~\ref{fig:exp1-time-vs-reg} \&~\ref{fig:exp2-time-vs-reg}
respectively, and described in~\Sref{sec:res-exp12}.

In Experiment~3, the 20~best-performing hyperparameter configurations
per each model family are used to train surrogates on sets of various sizes to
investigate their scaling properties. In particular, we consider sets of
sizes~$\{1,2,5,10,12,15,20\}$ in thousands of samples to better characterize the
relationship of each family between sample count and the metrics of interest
listed in~\Tref{tbl:metrics}.
The results of this experiment are shown in~\Fref{fig:scaling} and discussed
in~\Sref{sec:res-exp3}.

Finally, Experiment~4 aims
to produce surrogates suitable for practical use by retraining selected
well-scaling instances on large training sets in order to satisfy the goals of this work.
The results of this process are displayed in~\Fref{fig:reg-performance} and
in~\Tref{tbl:exp4-detailed-results}, and summarized in~\Sref{sec:res-exp4}.


\subsection{Adaptive Approach}\label{sec:adaptive}

Adaptive sampling techniques appear frequently in the literature and have been
specialized for surrogate modelling, where precision is implicitly limited by the quantity of training samples which are available from the expensive model. Garud's~\cite{Garud2016} ``Smart Sampling Algorithm'' achieved notable success by incorporating surrogate quality and
crowding distance scoring to identify optimal new samples, but was only tested
on a single-parameter domain. We theorized that a nondeterministic sample
generation approach, built around Markov Chain Monte Carlo methods (MCMC), would
fare better for high-dimensional models by more thoroughly exploring all local
optima in the feature space. MCMC produces each sample points according to a shared proposal
distribution from the prior point. These sample points will converge to a desired posterior
distribution, so long as the acceptance probability meets certain statistical criteria (see~\cite{Zhou2018} for a review).

\begin{figure}
	\centering
	\hspace*{-5pt}\includegraphics[width=1.28\linewidth,bb=0 0 1440 1380]{fig4_qassplan.png}
	\caption{\label{fig:qassplan}Schematic of QASS algorithm}
\end{figure}

Many researchers have embedded surrogate methods into MCMC strategies for
parameter optimisation~\cite{Zhang2020,Gong2017}, in particular the ASMO-PODE
algorithm~\cite{Ginting2011} which makes use of MCMC-based adaptive sampling. Our approach draws inspiration from ASMO-PODE, but instead uses MCMC to generate samples
which increase surrogate precision throughout the entire parameter space.

We designed the Quality-Adaptive Surrogate Sampling algorithm (QASS, \Fref{fig:qassplan}) to iteratively increment the training/test set with sample
points which maximize surrogate error and minimize a crowding distance metric
(CDM)~\cite{Solonen2012} in feature space. On each iteration following an initial training of the surrogate on $N$ uniformly random samples, the surrogate was trained and absolute error calculated. MCMC was then performed on the error function generated by performing nearest-neighbor interpolation on these test error points. The resultant samples were culled by $50\%$ according to the CDM, and then the $n$ highest-error candidates were selected for reintegration with the training/test set, beginning another training epoch. Validation was also performed during each iteration on independent, uniformly-random sample sets.
